{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from auto_login import get_driver_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def people_scrape(search_term, num_pages):  \n",
    "    loc_list = []\n",
    "    url_list = []\n",
    "    headline_list = []\n",
    "    name_list = []\n",
    "    current_job_list = []\n",
    "#                 function to automate search bar, search focus\n",
    "#                 from your linkedin homepage, collecting data from\n",
    "#                 search including name, location(secondary_deets), \n",
    "#                 headline(primary_deets) from condensed profiles returned from\n",
    "#                 search results. requires string entry for search term\n",
    "#                 and int input for number of pages (num_pages) \n",
    "#                 that you wish to scrape from results.\n",
    "\n",
    "    # activate search bar cursor with click\n",
    "    driver.find_element_by_css_selector(\"div#global-nav-search \").click()\n",
    "    time.sleep(2)\n",
    "    # send keyboard entry \"div[id='oc-background-section']\")for search terms\n",
    "    driver.find_element_by_css_selector(\"input.search-global-typeahead__input\").send_keys(search_term)\n",
    "    # send enter key to activate search\n",
    "    driver.find_element_by_css_selector(\"input.search-global-typeahead__input\").send_keys(Keys.RETURN)\n",
    "    # wait for results to load\n",
    "    driver.implicitly_wait(6)\n",
    "    #w.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.reusable-search__entity-results-list')))\n",
    "    element = driver.find_element_by_css_selector('ul.reusable-search__entity-results-list ')\n",
    "    # scroll to element containing target(people_banner)  allowing ajax elements to load\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
    "    driver.implicitly_wait(5)\n",
    "   # locate banner under search type results\n",
    "    people_banner = driver.find_element_by_link_text(f\"See all people results\")\n",
    "    # js function to click banner/button to see additional results under jobs, people, or posts                      \n",
    "    driver.execute_script('arguments[0].click();',people_banner)\n",
    "    # pause to allow page to load\n",
    "    driver.implicitly_wait(6)\n",
    "    # starting with pagination page 1 to increase as pagination occurs                     \n",
    "    page_number = 1\n",
    "\n",
    "    # set while loop to define pagination and data collection conditions\n",
    "    while page_number <= num_pages:\n",
    "        print(\"Processing page: \" + str(page_number))\n",
    "    # find all results on page\n",
    "        links = driver.find_elements_by_css_selector(\"div.entity-result__content \")\n",
    "   # pause for page load\n",
    "        time.sleep(2)\n",
    "    # iterate through results\n",
    "        for l in links:\n",
    "    # retrieve profile url\n",
    "            url = l.find_element_by_css_selector(\"span.entity-result__title a.app-aware-link\")\n",
    "    # add to urls list\n",
    "            url_list.append(str(url.get_attribute(\"pathname\")))\n",
    "     # locating elements containing text needed\n",
    "            details = l.find_elements_by_css_selector(\"div.linked-area\")\n",
    "    # the first element has the first three lines of text in the container\n",
    "            deets = details[0]  \n",
    "    # split text to assign elements appropriately\n",
    "            text = deets.get_attribute('innerText').split('\\n')\n",
    "    # retrieve name/add to list\n",
    "            name_list.append(text[0])\n",
    "    # retrieve location/add to list   \n",
    "            loc_list.append(text[-1])\n",
    "    # retrieve headline/add to list\n",
    "            headline_list.append(text[-2])\n",
    "    # the second element selected contains the 'Current:' job text \n",
    "            try:\n",
    "                current_job = details[1]\n",
    "    # removing the 'Current:' string from text\n",
    "                current_job_list.append(current_job.get_attribute('innerText').split(':')[1])\n",
    "            except (NoSuchElementException, IndexError):\n",
    "                current_job_list.append('nan')\n",
    "        time.sleep(3)\n",
    "        page_number+=1\n",
    "    # navigate using pagination function\n",
    "        if page_number < num_pages:\n",
    "            goto_next_page()\n",
    "    # print to verify page during processing\n",
    "            print(f\"attempting to navigate to search results page {page_number}\")\n",
    "            time.sleep(5)\n",
    "\n",
    "   \n",
    "    # create dataframe with extracted information and save as csv file\n",
    "    df = pd.DataFrame()                      \n",
    "    df['name'] = name_list\n",
    "    df['url'] = url_list\n",
    "    df['current_job'] = current_job_list\n",
    "    df['location'] = loc_list\n",
    "    df['headline'] = headline_list\n",
    "    # add complete url information for use in complete profile scraping\n",
    "    for row in df:\n",
    "        df['fetch'] = 'https://www.linkedin.com' + df.url + '/'\n",
    "    df.to_csv(f'{search_term}.csv')\n",
    "    # verify save\n",
    "    print(f'{search_term}.csv saved')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def people_scrape_by_start_page(search_term, start_page, num_pages):  \n",
    "    loc_list = []\n",
    "    url_list = []\n",
    "    headline_list = []\n",
    "    name_list = []\n",
    "    current_job_list = []\n",
    "#                 function to continue data scraping at page number\n",
    "#                 from your linkedin search, collecting data from\n",
    "#                 search including name, location(secondary_deets), \n",
    "#                 headline(primary_deets) from condensed profiles returned from\n",
    "#                 search results. requires string entry for search term\n",
    "#                 and int input for number of pages (num_pages) \n",
    "#                 that you wish to scrape from results.\n",
    "#                 start_page argument must be in integer format displayed in pagination bar\n",
    "#                 in active search\n",
    "\n",
    "\n",
    "    # starting with pagination page 1 to increase as pagination occurs                     \n",
    "    goto_page(start_page)\n",
    "    page_number = 1\n",
    "    \n",
    "    # set while loop to define pagination and data collection conditions\n",
    "    while page_number <= num_pages:\n",
    "        print(\"Processing page: \" + str(start_page))\n",
    "    # find all results on page\n",
    "        links = driver.find_elements_by_css_selector(\"div.entity-result__content \")\n",
    "   # pause for page load\n",
    "        time.sleep(2)\n",
    "    # iterate through results\n",
    "        for l in links:\n",
    "    # retrieve profile url\n",
    "            url = l.find_element_by_css_selector(\"span.entity-result__title a.app-aware-link\")\n",
    "    # add to urls list\n",
    "            url_list.append(str(url.get_attribute(\"pathname\")))\n",
    "     # locating elements containing text needed\n",
    "            details = l.find_elements_by_css_selector(\"div.linked-area\")\n",
    "    # the first element has the first three lines of text in the container\n",
    "            deets = details[0]  \n",
    "    # split text to assign elements appropriately\n",
    "            text = deets.get_attribute('innerText').split('\\n')\n",
    "    # retrieve name/add to list\n",
    "            name_list.append(text[0])\n",
    "    # retrieve location/add to list   \n",
    "            loc_list.append(text[-1])\n",
    "    # retrieve headline/add to list\n",
    "            headline_list.append(text[-2])\n",
    "    # the second element selected contains the 'Current:' job text \n",
    "            try:\n",
    "                current_job = details[1]\n",
    "    # removing the 'Current:' string from text\n",
    "                current_job_list.append(current_job.get_attribute('innerText').split(':')[1])\n",
    "            except (NoSuchElementException, IndexError):\n",
    "                current_job_list.append('nan')\n",
    "        time.sleep(3)\n",
    "        page_number+=1\n",
    "        start_page+=1\n",
    "    # navigate using pagination function\n",
    "        if page_number < num_pages:\n",
    "            goto_next_page()\n",
    "    # print to verify page during processing\n",
    "            print(f\"attempting to navigate to search results page {start_page}\")\n",
    "            time.sleep(5)\n",
    "\n",
    "   \n",
    "    # create dataframe with extracted information and save as csv file\n",
    "    df = pd.DataFrame()                      \n",
    "    df['name'] = name_list\n",
    "    df['url'] = url_list\n",
    "    df['current_job'] = current_job_list\n",
    "    df['location'] = loc_list\n",
    "    df['headline'] = headline_list\n",
    "    # add complete url information for use in complete profile scraping\n",
    "    for row in df:\n",
    "        df['fetch'] = 'https://www.linkedin.com' + df.url + '/'\n",
    "    df.to_csv(f'{search_term}.csv')\n",
    "    # verify save\n",
    "    print(f'{search_term}.csv saved')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to locate and interact with \"next\" button at bottom of search\n",
    "def goto_next_page():\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    w = WebDriverWait(driver, 15)\n",
    "    w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.artdeco-pagination.ember-view\")))\n",
    "    go_to_next = driver.find_element_by_css_selector('button[aria-label=\"Next\"]')\n",
    "    driver.execute_script('arguments[0].click();',go_to_next)\n",
    "    \n",
    "#function to locate and interact with \"next\" button at bottom of search\n",
    "def goto_page(page_number):\n",
    "    time.sleep(2)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    w = WebDriverWait(driver, 15)\n",
    "    w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.artdeco-pagination.ember-view\")))\n",
    "    pages = driver.find_element_by_css_selector('ul.artdeco-pagination__pages')\n",
    "    page_button = f'Page {page_number}'\n",
    "    target_page = driver.find_element_by_css_selector(f'button[aria-label=\"{page_button}\"]')\n",
    "    driver.execute_script('arguments[0].click();',target_page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experience():\n",
    "    time.sleep(3)\n",
    "    w = WebDriverWait(driver, 10)\n",
    "    w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.profile-detail\")))\n",
    "    h2_items = driver.find_elements_by_tag_name(\n",
    "        \"h2\")\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", h2_items[1])\n",
    "    background = driver.find_element_by_css_selector(\n",
    "        \"div#oc-background-section\")\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", background)\n",
    "    time.sleep(5)\n",
    "    # locate experience section element\n",
    "    exp = background.find_element_by_css_selector(\n",
    "                \"section#experience-section.pv-profile-section.experience-section.ember-view\")\n",
    "    # access individual job containers in list format \n",
    "    history = exp.find_elements_by_css_selector('li.pv-entity__position-group-pager')\n",
    "    job_count = len(history)\n",
    "    details = history[0]\n",
    "#          use of  try/except clause to locate element to avoid 'element not found' error which halts program\n",
    "# job title\n",
    "    try:\n",
    "        job = details.find_element_by_tag_name(\n",
    "                        'h3').get_attribute('innerText')\n",
    "    except NoSuchElementException:\n",
    "        job = 'nan'\n",
    " # company of employment\n",
    "    try:\n",
    "        company = details.find_element_by_tag_name(\n",
    "                        'p.pv-entity__secondary-title').get_attribute('innerText')\n",
    "        company = re.sub('Full-time', '', company)\n",
    "    except NoSuchElementException:\n",
    "        company = 'nan'\n",
    "# location of employment\n",
    "    try:\n",
    "        location = details.find_element_by_css_selector(\n",
    "                        'h4.pv-entity__location').get_attribute('innerText')\n",
    "        location = location.split('\\n', 1)[1]\n",
    "    except NoSuchElementException:\n",
    "        location = 'nan'\n",
    "# dates of employment\n",
    "    try:\n",
    "        date = details.find_element_by_css_selector(\n",
    "                        \"h4.pv-entity__date-range\").get_attribute('innerText').split(' ', 2)[-1]\n",
    "    except NoSuchElementException:\n",
    "        date = 'nan'\n",
    "        \n",
    "    return job_count, job, company, location, date\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email():\n",
    "#     function accessing and retrieves email and name from profile header, extracts first name \n",
    "    # scroll to top of profile where email is located\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(2)\n",
    "    header = driver.find_elements_by_css_selector(\n",
    "        \"ul.pv-top-card--list\")\n",
    "    # name on profile\n",
    "    name = header[0].get_attribute('innerText')\n",
    "   # first name derived from name\n",
    "    fname = name.split(' ')[0]\n",
    "    # find element with email contained within\n",
    "    contact_info = driver.find_element_by_css_selector(\n",
    "        'a[data-control-name=\"contact_see_more\"]')\n",
    "     # js function to click banner/button to see additional results under jobs, people, or posts                      \n",
    "    driver.execute_script('arguments[0].click();',contact_info)\n",
    "    time.sleep(3)\n",
    "    # not everyone provides email so try/except block used\n",
    "    try:\n",
    "        container =  driver.find_elements_by_css_selector(\n",
    "            'div.pv-contact-info__ci-container')\n",
    "        email = container[1].get_attribute('innerText')\n",
    "    except (NoSuchElementException, IndexError):\n",
    "        email = 'nan'\n",
    "# close pop up with contact info\n",
    "    close_popup = driver.find_element_by_css_selector(\n",
    "            'button[aria-label=\"Dismiss\" ]')\n",
    "    close_popup.click()\n",
    "    return name, fname, email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_connect(message):\n",
    "    # scroll to top of profile to ensure elements can be found by webdriver\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(5)\n",
    "    # locate & click 'Connect' button located at top of profile\n",
    "    try:\n",
    "        driver.find_element_by_class_name('pv-s-profile-actions--connect').click()\n",
    "        action_bar = driver.find_element_by_css_selector(\"div.artdeco-modal__actionbar\")\n",
    "    # locate 'Add a note' button by class\n",
    "        action_bar.find_element_by_class_name('mr1').click()\n",
    "        time.sleep(3)\n",
    "    # action to send text from message to input box\n",
    "        message_input = driver.find_element_by_id('custom-message').send_keys(message)\n",
    "        # send the message & connection request\n",
    "        driver.find_element_by_class_name('ml1').click()\n",
    "        \n",
    "    except (NoSuchElementException, ElementNotInteractableException):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_scrape(list_profile_urls):\n",
    "    job_count_list = []\n",
    "    job_list = []\n",
    "    company_list = []\n",
    "    date_list = []\n",
    "    location_list = []\n",
    "    names_list = []    \n",
    "    email_list = []\n",
    "    fname_list = []\n",
    "    for url in list_profile_urls:\n",
    "        driver.get(url)\n",
    "        job_count, job, company, location, date = get_experience()\n",
    "        job_count_list.append(job_count)\n",
    "        job_list.append(job)\n",
    "        company_list.append(company)\n",
    "        date_list.append(date)\n",
    "        location_list.append(location)\n",
    "        name, fname, email = get_email()\n",
    "        names_list.append(name)\n",
    "        fname_list.append(fname)\n",
    "        email_list.append(email)\n",
    "    df = pd.DataFrame()\n",
    "    df['profile_name'] = names_list\n",
    "    df['job1'] = job_list\n",
    "    df['job_count'] = job_count_list\n",
    "    df['company1'] = company_list\n",
    "    df['location1'] = location_list\n",
    "    df['dates1'] = date_list\n",
    "    df['first_name'] = fname_list\n",
    "    df['email'] = email_list\n",
    "    return df\n",
    "\n",
    "def make_connection(url_message_dict):\n",
    "    counter = 1\n",
    "\n",
    "    for key, value in url_message_dict.items():\n",
    "        url = key\n",
    "        message = value\n",
    "        driver.get(url)\n",
    "        profile_connect(message)\n",
    "        print('connection to :  ' +url+ ' is complete')\n",
    "        counter+=1\n",
    "    print('Process complete '+str(counter)+ ' connections requested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def search_scrape_connect(search_term, num_pages):   \n",
    "\n",
    "    search_df = people_scrape(search_term, num_pages)\n",
    "    \n",
    "    profile_urls = search_df['fetch']\n",
    "    \n",
    "    detail_df = profile_scrape(profile_urls)\n",
    "\n",
    "    df = pd.concat([search_df, detail_df], axis=1)\n",
    "    df = df.loc[df.company1 != 'nan']\n",
    "    df = df.loc[df.job1 != 'nan']\n",
    "\n",
    "    df['personalized_message'] = (\"Hi \" + df.first_name +\", I am a data scientist in the DC area. My background is in video editing. \"\n",
    "                                                       \"After completing the Flatiron data science program,  I am transitioning into the DS/ML career field\" \n",
    "                                                       \", hoping to segue into AI.\"\n",
    "                                                       \" I see that you are a \" + df.job1 + \"at \"+ df.company1+\n",
    "                                                       \", so I just wanted to reach out, connect, and say hello!\")  \n",
    "\n",
    "    url_message_dict = dict(zip(list(df.fetch), list(df.personalized_message)))\n",
    "    df.to_csv(f'{search_term}.csv')\n",
    "    print(f'initial {search_term}.csv overwritten, final csv file saved')\n",
    "    make_connection(url_message_dict)\n",
    "    return df\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_connect(search_df):   \n",
    "    profile_urls = search_df.fetch\n",
    "    detail_df = profile_scrape(profile_urls)\n",
    "\n",
    "    df = pd.concat([search_df, detail_df], axis=1)\n",
    "    df = df.loc[df.company1 != 'nan']\n",
    "    df = df.loc[df.job1 != 'nan']\n",
    "\n",
    "    df['personalized_message'] = (\"Hi \" + df.first_name +\", I am a data scientist in the DC area. My background is in video editing. \"\n",
    "                                                       \"After completing the Flatiron data science program,  I am transitioning into the DS/ML career field\" \n",
    "                                                       \", hoping to segue into AI.\"\n",
    "                                                       \" I see that you are a \" + df.job1 + \"at \"+ df.company1+\n",
    "                                                       \", so I just wanted to reach out, connect, and say hello!\")  \n",
    "\n",
    "    url_message_dict = dict(zip(list(df.fetch), list(df.personalized_message)))\n",
    "    df.to_csv('connections.csv')\n",
    "    print(f'initial {search_term}.csv overwritten, final csv file saved')\n",
    "    make_connection(url_message_dict)\n",
    "    return df\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilized in connect function to obtain name, job and company info for message without saving as csv\n",
    "def get_job_company():\n",
    "    time.sleep(3)\n",
    "    w = WebDriverWait(driver, 10)\n",
    "    w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.profile-detail\")))\n",
    "    h2_items = driver.find_elements_by_tag_name(\n",
    "        \"h2\")\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", h2_items[1])\n",
    "    background = driver.find_element_by_css_selector(\n",
    "        \"div#oc-background-section\")\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", background)\n",
    "    time.sleep(6)\n",
    "    # locate experience section element\n",
    "    exp = background.find_element_by_css_selector(\n",
    "                \"section#experience-section.pv-profile-section.experience-section.ember-view\")\n",
    "    # access individual job containers in list format \n",
    "    history = exp.find_elements_by_css_selector('li.pv-entity__position-group-pager')\n",
    "    job_count = len(history)\n",
    "    details = history[0]\n",
    "#          use of  try/except clause to locate element to avoid 'element not found' error which halts program\n",
    "# job title\n",
    "    try:\n",
    "        job = details.find_element_by_tag_name(\n",
    "                        'h3').get_attribute('innerText')\n",
    "    except NoSuchElementException:\n",
    "        job = 'nan'\n",
    " # company of employment\n",
    "    try:\n",
    "        company = details.find_element_by_tag_name(\n",
    "                        'p.pv-entity__secondary-title').get_attribute('innerText')\n",
    "        company = re.sub('Full-time', '', company)\n",
    "    except NoSuchElementException:\n",
    "        company = 'nan'\n",
    "    \n",
    "    job_info = [job, company]\n",
    "    return job_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_name():\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(2)\n",
    "    header = driver.find_elements_by_css_selector(\n",
    "            \"ul.pv-top-card--list\")\n",
    "    # name on profile\n",
    "    header_text = header[0].get_attribute('innerText')\n",
    "   # first name derived from name\n",
    "    fname = header_text.split(' ')[0]\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_name(search_df):   \n",
    "# use to complete connection process when timeout errors occur mid process for scrape part of function\n",
    "#  use df obtained from search function as search_df \n",
    "# use this if df names are incomplete or if only urls are present, this function includes the get_first_name function to fill message\n",
    "    profile_urls = search_df.fetch\n",
    "    for url in profile_urls:\n",
    "        driver.get(url)\n",
    "        fname = get_first_name()\n",
    "        job_info = get_job_company()\n",
    "        current_job = job_info[0]\n",
    "        current_company = job_info[1]\n",
    "        \n",
    "        if current_job != 'nan':\n",
    "            if current_company != 'nan':\n",
    "                message = (\"Hi \" + fname +\", I am a data scientist in the DC area. My background is in video editing. \",\n",
    "                                                               \"I am transitioning into the DS/ML career field, hoping to segue into AI.\", \n",
    "                                                               \" How do you like your role as \" + current_job + \" at \"+ current_company+\"?\",\n",
    "                                                               \"Anyway, I just wanted to reach out, connect, and say hello!\")  \n",
    "\n",
    "\n",
    "                profile_connect(message)\n",
    "                print(f'Connection complete: {fname} // {current_company} // {current_job} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(search_df):   \n",
    "# use to complete connection process when timeout errors occur mid process for scrape part of function\n",
    "#  use df obtained from search function as search_df \n",
    "# uses name acquired in people_search to complete message\n",
    "    url_name_dict = dict(zip(list(search_df.name), list(search_df.fetch)))\n",
    "    for name, url in url_name_dict.items():\n",
    "        driver.get(url)\n",
    "   # first name derived from name\n",
    "        fname = name.split(' ')[0]\n",
    "\n",
    "        job_info = get_job_company()\n",
    "        current_job = job_info[0]\n",
    "        current_company = job_info[1]\n",
    "        if current_job != 'nan':\n",
    "            if current_company != 'nan':\n",
    "                message = (\"Hi \" + fname +\", I am a data scientist in the DC area. My background is in video editing. \",\n",
    "                                                               \"I am transitioning into the DS/ML career field, hoping to segue into AI.\", \n",
    "                                                               \" How do you like your role as \" + current_job + \" at \"+ current_company+\"?\",\n",
    "                                                               \"Anyway, I just wanted to reach out, connect, and say hello!\")  \n",
    "\n",
    "\n",
    "                profile_connect(message)\n",
    "                print(f'Connection complete: {name} // {current_company} // {current_job} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver =  get_driver_login('email@gmail.com', 'password1!', PATH = \"C:\\Program Files (x86)\\chromedriver.exe\")\n",
    "# does not return second csv with more details like email, location, dates, etc but tends to run smoother \n",
    "search_df = people_scrape('data scientist', 3)\n",
    "connect(search_df)\n",
    "# does return second csv with more details like email, location, dates, etc, connect within function\n",
    "search_scrape_connect('data scientist', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page: 1\n",
      "attempting to navigate to search results page 2\n",
      "Processing page: 2\n",
      "attempting to navigate to search results page 3\n",
      "Processing page: 3\n",
      "attempting to navigate to search results page 4\n",
      "Processing page: 4\n",
      "attempting to navigate to search results page 5\n",
      "Processing page: 5\n",
      "attempting to navigate to search results page 6\n",
      "Processing page: 6\n",
      "attempting to navigate to search results page 7\n",
      "Processing page: 7\n",
      "attempting to navigate to search results page 8\n",
      "Processing page: 8\n",
      "attempting to navigate to search results page 9\n",
      "Processing page: 9\n",
      "Processing page: 10\n",
      "ai engineer.csv saved\n"
     ]
    }
   ],
   "source": [
    "df = people_scrape('ai engineer', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page: 65\n",
      "attempting to navigate to search results page 66\n",
      "Processing page: 66\n",
      "attempting to navigate to search results page 67\n",
      "Processing page: 67\n",
      "attempting to navigate to search results page 68\n",
      "Processing page: 68\n",
      "attempting to navigate to search results page 69\n",
      "Processing page: 69\n",
      "attempting to navigate to search results page 70\n",
      "Processing page: 70\n",
      "attempting to navigate to search results page 71\n",
      "Processing page: 71\n",
      "attempting to navigate to search results page 72\n",
      "Processing page: 72\n",
      "attempting to navigate to search results page 73\n",
      "Processing page: 73\n",
      "attempting to navigate to search results page 74\n",
      "Processing page: 74\n",
      "attempting to navigate to search results page 75\n",
      "Processing page: 75\n",
      "attempting to navigate to search results page 76\n",
      "Processing page: 76\n",
      "attempting to navigate to search results page 77\n",
      "Processing page: 77\n",
      "attempting to navigate to search results page 78\n",
      "Processing page: 78\n",
      "attempting to navigate to search results page 79\n",
      "Processing page: 79\n",
      "attempting to navigate to search results page 80\n",
      "Processing page: 80\n",
      "attempting to navigate to search results page 81\n",
      "Processing page: 81\n",
      "attempting to navigate to search results page 82\n",
      "Processing page: 82\n",
      "attempting to navigate to search results page 83\n",
      "Processing page: 83\n",
      "Processing page: 84\n",
      "data engineer5.csv saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>current_job</th>\n",
       "      <th>location</th>\n",
       "      <th>headline</th>\n",
       "      <th>fetch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carl Baumbach</td>\n",
       "      <td>/in/carlbaumbach</td>\n",
       "      <td>...and Hadoop 2.6.5+ Graduated with a Master’...</td>\n",
       "      <td>Washington DC-Baltimore Area</td>\n",
       "      <td>Data Engineer at AE Strategies</td>\n",
       "      <td>https://www.linkedin.com/in/carlbaumbach/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medha Banda</td>\n",
       "      <td>/in/medha-banda-8b5210159</td>\n",
       "      <td>Data Engineer at TISTA Science and Technology...</td>\n",
       "      <td>Potomac, MD</td>\n",
       "      <td>Data Engineer| Hadoop| Bigdata</td>\n",
       "      <td>https://www.linkedin.com/in/medha-banda-8b5210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brian Duffy</td>\n",
       "      <td>/in/bduffy0329</td>\n",
       "      <td>Data Engineer at LMI</td>\n",
       "      <td>Washington DC-Baltimore Area</td>\n",
       "      <td>Senior Data Engineer at LMI</td>\n",
       "      <td>https://www.linkedin.com/in/bduffy0329/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haseeb Ahamed Naseem</td>\n",
       "      <td>/in/haseebahamed</td>\n",
       "      <td>Lead Data Engineer at Capital One</td>\n",
       "      <td>Falls Church, VA</td>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>https://www.linkedin.com/in/haseebahamed/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kavyashree Anantha Raman</td>\n",
       "      <td>/in/kavyashreeanantharaman</td>\n",
       "      <td>Data Mining Engineer at Hughes Network Systems</td>\n",
       "      <td>Washington DC-Baltimore Area</td>\n",
       "      <td>Data Analytics Engineer</td>\n",
       "      <td>https://www.linkedin.com/in/kavyashreeananthar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Annette C.</td>\n",
       "      <td>/in/annettechun</td>\n",
       "      <td>Software Engineer at Comscore, Inc.</td>\n",
       "      <td>Washington DC-Baltimore Area</td>\n",
       "      <td>Data Engineer at Amazon Web Services (AWS)</td>\n",
       "      <td>https://www.linkedin.com/in/annettechun/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Sarah El-Sharkawi</td>\n",
       "      <td>/in/sarah-el-sharkawi-335a6a117</td>\n",
       "      <td>Data &amp; Analytics Consultant at EY - Analyzed ...</td>\n",
       "      <td>Leesburg, VA</td>\n",
       "      <td>Senior Data Engineer at EY</td>\n",
       "      <td>https://www.linkedin.com/in/sarah-el-sharkawi-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>David Yu</td>\n",
       "      <td>/in/david-yu-07aa6539</td>\n",
       "      <td>Data Analyst at Comscore, Inc.</td>\n",
       "      <td>Washington DC-Baltimore Area</td>\n",
       "      <td>Data Engineer at Accenture Federal Services</td>\n",
       "      <td>https://www.linkedin.com/in/david-yu-07aa6539/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Nirmalkumar Elumalai</td>\n",
       "      <td>/in/nirmalkumar-elumalai-06b6a9178</td>\n",
       "      <td>Senior Data Engineer at Cognizant</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>Data Engineer at Cognizant</td>\n",
       "      <td>https://www.linkedin.com/in/nirmalkumar-elumal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Andy Kim</td>\n",
       "      <td>/in/andy-kim-40032395</td>\n",
       "      <td>Senior Data Engineer at Capital One - ...ente...</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>Senior Data Engineer at Capital One</td>\n",
       "      <td>https://www.linkedin.com/in/andy-kim-40032395/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name                                 url  \\\n",
       "0               Carl Baumbach                    /in/carlbaumbach   \n",
       "1                 Medha Banda           /in/medha-banda-8b5210159   \n",
       "2                 Brian Duffy                      /in/bduffy0329   \n",
       "3        Haseeb Ahamed Naseem                    /in/haseebahamed   \n",
       "4    Kavyashree Anantha Raman          /in/kavyashreeanantharaman   \n",
       "..                        ...                                 ...   \n",
       "169                Annette C.                     /in/annettechun   \n",
       "170         Sarah El-Sharkawi     /in/sarah-el-sharkawi-335a6a117   \n",
       "171                  David Yu               /in/david-yu-07aa6539   \n",
       "172      Nirmalkumar Elumalai  /in/nirmalkumar-elumalai-06b6a9178   \n",
       "173                  Andy Kim               /in/andy-kim-40032395   \n",
       "\n",
       "                                           current_job  \\\n",
       "0     ...and Hadoop 2.6.5+ Graduated with a Master’...   \n",
       "1     Data Engineer at TISTA Science and Technology...   \n",
       "2                                 Data Engineer at LMI   \n",
       "3                    Lead Data Engineer at Capital One   \n",
       "4       Data Mining Engineer at Hughes Network Systems   \n",
       "..                                                 ...   \n",
       "169                Software Engineer at Comscore, Inc.   \n",
       "170   Data & Analytics Consultant at EY - Analyzed ...   \n",
       "171                     Data Analyst at Comscore, Inc.   \n",
       "172                  Senior Data Engineer at Cognizant   \n",
       "173   Senior Data Engineer at Capital One - ...ente...   \n",
       "\n",
       "                         location  \\\n",
       "0    Washington DC-Baltimore Area   \n",
       "1                     Potomac, MD   \n",
       "2    Washington DC-Baltimore Area   \n",
       "3                Falls Church, VA   \n",
       "4    Washington DC-Baltimore Area   \n",
       "..                            ...   \n",
       "169  Washington DC-Baltimore Area   \n",
       "170                  Leesburg, VA   \n",
       "171  Washington DC-Baltimore Area   \n",
       "172                    McLean, VA   \n",
       "173                    McLean, VA   \n",
       "\n",
       "                                        headline  \\\n",
       "0                 Data Engineer at AE Strategies   \n",
       "1                 Data Engineer| Hadoop| Bigdata   \n",
       "2                    Senior Data Engineer at LMI   \n",
       "3                             Lead Data Engineer   \n",
       "4                        Data Analytics Engineer   \n",
       "..                                           ...   \n",
       "169   Data Engineer at Amazon Web Services (AWS)   \n",
       "170                   Senior Data Engineer at EY   \n",
       "171  Data Engineer at Accenture Federal Services   \n",
       "172                   Data Engineer at Cognizant   \n",
       "173          Senior Data Engineer at Capital One   \n",
       "\n",
       "                                                 fetch  \n",
       "0            https://www.linkedin.com/in/carlbaumbach/  \n",
       "1    https://www.linkedin.com/in/medha-banda-8b5210...  \n",
       "2              https://www.linkedin.com/in/bduffy0329/  \n",
       "3            https://www.linkedin.com/in/haseebahamed/  \n",
       "4    https://www.linkedin.com/in/kavyashreeananthar...  \n",
       "..                                                 ...  \n",
       "169           https://www.linkedin.com/in/annettechun/  \n",
       "170  https://www.linkedin.com/in/sarah-el-sharkawi-...  \n",
       "171     https://www.linkedin.com/in/david-yu-07aa6539/  \n",
       "172  https://www.linkedin.com/in/nirmalkumar-elumal...  \n",
       "173     https://www.linkedin.com/in/andy-kim-40032395/  \n",
       "\n",
       "[174 rows x 6 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_scrape_by_start_page('ai engineer2', 11, 20)\n",
    "people_scrape_by_start_page('ai engineer3', 31, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(input_string):\n",
    "    encoded_string = ''\n",
    "    prev_char = ''\n",
    "    count = 1\n",
    "    if not input_string: return ''\n",
    "    for char in input_string:\n",
    "        if char != prev_char:\n",
    "            if prev_char:\n",
    "                encoded_string += str(count) + prev_char\n",
    "            count = 1\n",
    "            prev_char = char\n",
    "        else:\n",
    "            count += 1\n",
    "    else:\n",
    "        encoded_string += str(count) + prev_char\n",
    "        return encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'6A1F2D2C1k1l1g1j1f1d12E'\n"
     ]
    }
   ],
   "source": [
    "encoded_val = rle_encode('AAAAAAFDDCCklgjfdEEEEEEEEEEEE')\n",
    "print(encoded_val.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(input_string):\n",
    "    encoded_string = ''\n",
    "    prev_char = ''\n",
    "    count = 1\n",
    "\n",
    "    for char in input_string:\n",
    "        if char != prev_char:\n",
    "            if prev_char:\n",
    "                encoded_string += str(count) + prev_char\n",
    "                count = 1\n",
    "                prev_char = char\n",
    "            else:\n",
    "                count += 1\n",
    "        else:\n",
    "            encoded_string += str(count) + prev_char\n",
    "    return encoded_string.encode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compress(input_string):\n",
    "    encoded_string = ''\n",
    "    prev_char = ''\n",
    "    count = 1\n",
    "    if not input_string:\n",
    "        return ''.encode()\n",
    "    for char in input_string:\n",
    "        if char != prev_char:\n",
    "            if prev_char:\n",
    "                encoded_string += str(count) + prev_char\n",
    "                count = 1\n",
    "                prev_char = char\n",
    "        else:\n",
    "            count += 1\n",
    "    else:\n",
    "        encoded_string += str(count) + prev_char\n",
    "        return encoded_string.encode(\"utf-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xff\\xfe1\\x00'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress('AAAAAAFDDCCklgjfdEEEEEEEEE EEE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'a1'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"a1\".encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_for_loop(str_input):\n",
    "    encoded_string, prev_char, count = '', '', 1\n",
    "    if not str_input: \n",
    "        return ''.encode()\n",
    "    for char in str_input:\n",
    "        if char != prev_char:\n",
    "            if prev_char:\n",
    "                encoded_string += str(count) + prev_char\n",
    "                count=1\n",
    "            else:\n",
    "                encoded_string += str(count) + char\n",
    "        prev_char = char\n",
    "    return encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_while_loop(str_input):\n",
    "    encoded_string = \"\"\n",
    "    j = 0\n",
    "    while j < len(str_input):\n",
    "        count = 1\n",
    "        while j + 1 < len(str_input) & str_input[j] = string_input[j + 1]:\n",
    "            count += 1\n",
    "            j += 1\n",
    "        encoded_string += str(count) + string_input[j]\n",
    "        j += 1\n",
    "    return encoded_string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def rle_counter(str_input):\n",
    "    encoded_string = \"\"\n",
    "    encode_dict = OrderedDict.fromkeys(str_input, 0)\n",
    "   # iterating over the string\n",
    "    for char in str_input:\n",
    "      # incrementing the frequency\n",
    "      encode_dict[char] += 1\n",
    "    \n",
    "    for key, value in encode_dict.items():\n",
    "        encoded_string += key + str(value)\n",
    "    return encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k3j1'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rle_encode_counter('kjkk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "def rle_lc(str_input):\n",
    "    grouped = [list(g) for k, g in groupby(str_input)]\n",
    "    return ''.join(['{}{}'.format(k, sum(1 for _ in g)) for k, g in groupby(str_input)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k1j1k2'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rle_encode_lc('kjkk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_gen(data: str) -> str:\n",
    "    # returns run length encoded string for data\n",
    "    return \"\".join(f\"{x}{sum(1 for _ in y)}\" for x, y in groupby(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k1j1k2'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rle_encode_gen('kjkk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
